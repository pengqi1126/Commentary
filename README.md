
## 🚀 快速开始

### 前置要求

- Python 3.8+
- FFmpeg（视频处理）
- OpenAI API 密钥（或其他LLM服务）

### 安装步骤
**克隆仓库**
   ```bash
   git clone https://github.com/pengqi1126/Commentary.git
   cd Commentary

### 结构

1. 视频处理层 🎥
video_io.py - 视频读取器
功能：视频文件的底层输入/输出操作

核心方法：get_frame(second) - 精确获取视频第 X 秒的画面

职责：处理不同视频格式，管理视频流，提供精确的时间戳定位

frame_sampler.py - 关键帧采样器
功能：智能选择需要分析的关键帧

输入：视频时间线

输出：一组关键的 frames 对象

策略：根据场景变化率、重要事件点等动态决定采样频率

2. 内容分析层 🔍
frame_analyzer.py - 视觉分析引擎
功能：将画面转换为文字描述

流程：接收帧 → 调用视觉模型 → 生成自然语言描述

输出：如"球员带球突破至禁区"、"蓝色队伍组织进攻"等描述

qwen-client.py - 通义千问模型客户端
功能：连接阿里云通义千问-Plus 模型

输入：JPG 图片 + 自定义提示词（prompt）

输出：针对画面的详细文字描述（str）

特色：支持多模态输入，理解画面中的动作、情感、战术等

3. 数据整合层 📊
video_to_description_mapper.py - 事件映射器
功能：将视频片段与专业解说数据关联

关键：以 clip 文件名反查 PBP JSON（Play-by-Play，逐播报）文件

输出：event_info - 包含事件类型、参与者、时间等结构化数据

示例：篮球比赛中的"投篮"、"助攻"、"犯规"等事件

scene_builder.py - 场景构造器
功能：融合视觉内容与事件数据，构建完整场景

输入：frames 描述 + event_info

输出：scene 对象 - 包含时间线、视觉内容、事件数据的复合结构

4. 内容生成层 ✍️
prompt_builder.py - 提示词工程师
功能：根据场景类型生成优化的 LLM 提示词

输入：scene 对象

输出：针对不同 event_type 的专业解说 prompt

策略：

精彩时刻：激昂、情感丰富的解说

战术分析：专业、技术性强的描述

普通事件：流畅、自然的播报

main.py - 总控与中文解说生成
功能：项目主控制器，协调所有模块

核心流程：

接收场景和 prompt

调用 LLM 生成中文解说文本

管理整个处理流程的时序

特色：确保解说符合中文播音风格，添加适当的语气词和衔接语

5. 输出合成层 🔊
tts_engine.py - 语音合成器
功能：将文字解说转换为自然语音

技术：使用 qwen_tts 引擎

控制：语音的语速、语调、情感色彩

输出：与解说文本同步的音频文件

merge.py - 多媒体合成器
功能：将各个组件合并为最终成品

合成内容：

原始/处理后的视频流

生成的解说语音

字幕/图形叠加（如需要）

输出：可直接分发的完整解说视频

🔄 完整数据处理流程
输入：原始视频 + PBP JSON（专业赛事数据）

抽取：获取关键帧 → 分析画面内容 → 关联专业事件数据

理解：构建场景对象 → 生成针对性提示词

创作：LLM生成中文解说 → TTS转为语音

合成：语音+视频+字幕合并输出
